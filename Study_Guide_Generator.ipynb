{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pakeetharan/ai-study-guide/blob/main/Study_Guide_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö AI Study Guide Generator\n",
        "**Turn your lecture slides and textbooks into professional exam notes and practice questions.**\n",
        "\n",
        "### **How to use this tool:**\n",
        "1. **Check Settings:** Go to top menu `Runtime` -> `Change runtime type` and ensure **T4 GPU** is selected.\n",
        "2. **Initialize:** Click the **Play** button on **Step 1** below. Wait for it to say \"System Ready\" (~2 mins).\n",
        "3. **Upload & Run:** Click the **Play** button on **Step 2**.\n",
        "    * You will be asked to connect to **Google Drive** (this is to safely save your final PDF).\n",
        "    * Click **\"Choose Files\"** to upload your PDFs. You can upload multiple files (e.g., *Week1.pdf, Week2.pdf*) at once.\n",
        "4. **Get Results:** The AI will analyze each document separately and save a `Study_Guide_TIMESTAMP.pdf` into your Google Drive folder: `My Drive > AI_Study_Notes`.\n",
        "\n",
        "---\n",
        "**üí° Pro Tip:** Upload separate PDF files for each lecture topic instead of merging them. This helps the AI generate specific practice questions for every single topic."
      ],
      "metadata": {
        "id": "1Gm2N9Znbrc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üöÄ Step 1: Initialize System\n",
        "# @markdown Installs the AI engine, OCR tools, and PDF processors. It takes about **2 minutes**.\n",
        "# @markdown You only need to run this once per session.\n",
        "\n",
        "import os, sys, subprocess\n",
        "import logging, warnings\n",
        "\n",
        "# Memory Config\n",
        "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"‚è≥ Installing System Dependencies...\")\n",
        "with open(os.devnull, 'w') as devnull:\n",
        "    subprocess.run([\"apt-get\", \"update\"], stdout=devnull, stderr=devnull)\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"tesseract-ocr\", \"poppler-utils\",\n",
        "                    \"libcairo2\", \"libpango-1.0-0\", \"libgdk-pixbuf2.0-0\", \"libffi-dev\",\n",
        "                    \"fonts-roboto\", \"fonts-liberation\"], stdout=devnull, stderr=devnull)\n",
        "\n",
        "    pkgs = [\n",
        "        \"transformers\", \"accelerate\", \"bitsandbytes\", \"langchain-huggingface\",\n",
        "        \"langchain-text-splitters\", \"langchain-community\", \"langchain-core\",\n",
        "        \"pdfplumber\", \"pdf2image\", \"pytesseract\", \"markdown\", \"weasyprint\",\n",
        "        \"tiktoken\", \"tqdm\", \"numpy\"\n",
        "    ]\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs, stdout=devnull, stderr=devnull)\n",
        "\n",
        "import torch\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "import markdown\n",
        "import re\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from pdf2image import convert_from_path\n",
        "from google.colab import files, drive\n",
        "from weasyprint import HTML, CSS\n",
        "from weasyprint.text.fonts import FontConfiguration\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "print(\"‚è≥ Loading AI Model...\")\n",
        "model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Smart Offloading Configuration\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    max_memory={0: \"13GB\", \"cpu\": \"24GB\"},\n",
        "    offload_folder=\"offload\"\n",
        ")\n",
        "\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048, model_kwargs={\"temperature\": 0.3}, return_full_text=False)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"‚úÖ System Ready.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E8Z2sbtTb67n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìÇ Step 2: Upload Files & Generate Guide\n",
        "# @markdown **Instructions:**\n",
        "# @markdown 1. Run this cell to connect to Drive.\n",
        "# @markdown 2. Upload your PDFs when the button appears.\n",
        "# @markdown 3. The AI will process each file and save the result to `My Drive > AI_Study_Notes`.\n",
        "\n",
        "import os\n",
        "from google.colab import drive, files\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# --- 1. Connect Drive ---\n",
        "print(\"üîå Checking Google Drive connection...\")\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "output_folder = \"/content/drive/My Drive/AI_Study_Notes\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "generate_exercises = True # @param {type:\"boolean\"}\n",
        "OPTIMAL_CHUNK_SIZE = 6500\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "# --- 3. Helper Functions ---\n",
        "def clean_text_formatting(text):\n",
        "    \"\"\"\n",
        "    Fixes common AI formatting errors:\n",
        "    1. Removes unnecessary code blocks around plain text.\n",
        "    2. Ensures sequential numbering (1. 2. 3. instead of 1. 1. 1.)\n",
        "    \"\"\"\n",
        "    # Remove ``` block if it wraps the entire response or simple text\n",
        "    # But keep it if it looks like actual code/math\n",
        "    if text.strip().startswith(\"```\") and \"def \" not in text and \"=\" not in text:\n",
        "        text = text.replace(\"```markdown\", \"\").replace(\"```\", \"\")\n",
        "\n",
        "    # Fix broken list numbering (e.g. 1. Question 1. Question -> 1. Question 2. Question)\n",
        "    lines = text.split('\\n')\n",
        "    new_lines = []\n",
        "    question_count = 1\n",
        "\n",
        "    for line in lines:\n",
        "        # Detect lines starting with \"1.\" that act as headers\n",
        "        if re.match(r'^\\d+\\.', line.strip()) and len(line) > 5:\n",
        "            line = re.sub(r'^\\d+\\.', f'{question_count}.', line, 1)\n",
        "            question_count += 1\n",
        "        new_lines.append(line)\n",
        "\n",
        "    return '\\n'.join(new_lines)\n",
        "\n",
        "def extract_text_from_file(filename):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(filename) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                extracted = page.extract_text()\n",
        "                if extracted: text += extracted + \"\\n\"\n",
        "        if len(text) < 500:\n",
        "            print(f\"   ‚ö†Ô∏è Scanned content detected in {filename}. Running OCR...\")\n",
        "            images = convert_from_path(filename)\n",
        "            for img in images: text += pytesseract.image_to_string(img) + \"\\n\"\n",
        "    except Exception as e: print(f\"   ‚ùå Error reading {filename}: {e}\")\n",
        "    return text\n",
        "\n",
        "def run_pipeline():\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"   ‚¨áÔ∏è  CLICK THE BUTTON BELOW TO UPLOAD  ‚¨áÔ∏è\")\n",
        "    print(\"=\"*40)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No files uploaded.\")\n",
        "        return\n",
        "\n",
        "    all_notes_markdown = \"\"\n",
        "    all_exercises_markdown = \"\"\n",
        "\n",
        "    for i, filename in enumerate(uploaded.keys()):\n",
        "        print(f\"\\nüöÄ Processing File {i+1}/{len(uploaded)}: {filename}...\")\n",
        "\n",
        "        raw_text = extract_text_from_file(filename)\n",
        "        if not raw_text.strip(): continue\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "            tokenizer, chunk_size=OPTIMAL_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "        docs = splitter.create_documents([raw_text])\n",
        "\n",
        "        # --- Generate Academic Notes ---\n",
        "        print(f\"   üìù Generating Academic Notes ({len(docs)} sections)...\")\n",
        "        file_notes = f\"# Module: {filename}\\n\"\n",
        "\n",
        "        note_prompt = PromptTemplate.from_template(\n",
        "            \"\"\"\n",
        "            You are an expert Professor creating a comprehensive Study Guide. Analyze the following text:\n",
        "            \"{text}\"\n",
        "\n",
        "            ### 1. Analyze the Content Type\n",
        "            Determine the best structure for these notes based on the content:\n",
        "            - **If History/Narrative:** Use Timelines or Sequence flows.\n",
        "            - **If Technical/Scientific:** Use Definitions, Formulas, and Process Steps.\n",
        "            - **If Comparative:** Use Comparison Tables (e.g., X vs Y).\n",
        "            - **If Code/Programming:** Use Syntax Blocks and Explanations.\n",
        "\n",
        "            ### 2. Generate the Notes (Strict Visual Styling)\n",
        "            Produce the study guide using the adaptive structure you chose above. You MUST follow these visual rules:\n",
        "\n",
        "            * **Titles:** Use `# Main Topic` for the document title (only once).\n",
        "            * **Headers:** Use `## Section Header` for major sections and `### Sub-header` for subsections.\n",
        "            * **Key Terms:** Always **bold** important terms when first defined.\n",
        "            * **Lists:** Use standard bullet points (`- `) or numbered lists (`1. `) where appropriate.\n",
        "            * **Tables:** If comparing items or listing data, YOU MUST use a Markdown table.\n",
        "            * **Code/Math:** Use fenced code blocks (```) for all formulas, equations, or code snippets.\n",
        "            * **Callouts:** Use Blockquotes (`>`) for critical warnings, important notes, or \"Remember This\" tips.\n",
        "\n",
        "            ### 3. Final Summary\n",
        "            End with a `### Summary` section containing 3-5 high-level takeaways.\n",
        "\n",
        "            **Output Requirement:** Return ONLY the formatted Markdown. No conversational filler.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        for doc in tqdm(docs, desc=\"   > Analyzing\", leave=False):\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": note_prompt.format(text=doc.page_content)}]\n",
        "                fmt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "                # Rely on smart offloading for memory safety\n",
        "                res = pipe(fmt, max_new_tokens=1500, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"]\n",
        "                clean_res = res.split(\"assistant\")[-1].strip() if \"assistant\" in res else res\n",
        "                file_notes += f\"\\n{clean_res}\\n\"\n",
        "            except: pass\n",
        "\n",
        "        all_notes_markdown += file_notes + \"\\n\\n<div class='page-break'></div>\\n\\n\"\n",
        "\n",
        "        if generate_exercises:\n",
        "            print(f\"   üß† Designing Exam Questions...\")\n",
        "            mid = len(raw_text) // 3\n",
        "            sample_context = raw_text[mid : mid + OPTIMAL_CHUNK_SIZE]\n",
        "\n",
        "            ex_prompt = f\"\"\"\n",
        "            Create a Practice Exam based on this text:\n",
        "            \"{sample_context}\"\n",
        "\n",
        "            STRICT FORMATTING RULES:\n",
        "\n",
        "            ## Practice Exam: {filename}\n",
        "\n",
        "            ### Multiple Choice\n",
        "            1. [Question text here?]\n",
        "               a) [Option]\n",
        "               b) [Option]\n",
        "               c) [Option]\n",
        "               d) [Option]\n",
        "\n",
        "            2. [Next Question?]\n",
        "               a) [Option]...\n",
        "\n",
        "            ### Short Answer\n",
        "            3. [Question text?]\n",
        "\n",
        "            ### Answer Key\n",
        "            > 1. a) [Brief explanation]\n",
        "            > 2. b) [Brief explanation]\n",
        "            > 3. [Brief Answer]\n",
        "\n",
        "            DO NOT use Code Blocks for the Questions. Use standard text. MCQ options should be in New Line\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": ex_prompt}]\n",
        "                fmt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "                res = pipe(fmt, max_new_tokens=1500, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"].split(\"assistant\")[-1].strip()\n",
        "\n",
        "                # Clean formatting (fix numbering, remove rogue code blocks)\n",
        "                res = clean_text_formatting(res)\n",
        "\n",
        "                all_exercises_markdown += f\"{res}\\n\\n<div class='page-break'></div>\\n\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error generating exercises: {e}\")\n",
        "\n",
        "    # --- Final PDF Assembly ---\n",
        "    print(\"\\nüíæ Rendering Final PDF...\")\n",
        "    final_md = f\"\"\"\n",
        "    {all_notes_markdown}\n",
        "    # Part 2: Practice Workbook\n",
        "    {all_exercises_markdown}\n",
        "    \"\"\"\n",
        "\n",
        "    html_content = markdown.markdown(final_md, extensions=['extra', 'codehilite', 'tables', 'fenced_code'])\n",
        "\n",
        "    # CSS: Academic Standard\n",
        "    css = CSS(string=\"\"\"\n",
        "        @page { size: A4; margin: 2.5cm; }\n",
        "        body {\n",
        "            font-family: 'Roboto', 'Helvetica', sans-serif;\n",
        "            font-size: 11pt;\n",
        "            line-height: 1.6;\n",
        "            color: #333;\n",
        "        }\n",
        "\n",
        "        /* Headers - Consistent Styling */\n",
        "        h1 {\n",
        "            color: #2c3e50;\n",
        "            border-bottom: 2px solid #2c3e50;\n",
        "            padding-bottom: 10px;\n",
        "            margin-top: 40px;\n",
        "            font-size: 20pt;\n",
        "        }\n",
        "        h2 {\n",
        "            color: #2980b9;\n",
        "            margin-top: 30px;\n",
        "            font-size: 16pt;\n",
        "            border-left: 4px solid #2980b9;\n",
        "            padding-left: 10px;\n",
        "        }\n",
        "        h3 {\n",
        "            color: #e67e22;\n",
        "            font-size: 13pt;\n",
        "            margin-top: 20px;\n",
        "        }\n",
        "\n",
        "        /* Components */\n",
        "        table {\n",
        "            width: 100%;\n",
        "            border-collapse: collapse;\n",
        "            margin: 20px 0;\n",
        "            font-size: 10pt;\n",
        "        }\n",
        "        th { background-color: #ecf0f1; color: #2c3e50; padding: 8px; border: 1px solid #bdc3c7; }\n",
        "        td { border: 1px solid #bdc3c7; padding: 8px; }\n",
        "\n",
        "        pre {\n",
        "            background: #f8f9fa;\n",
        "            padding: 15px;\n",
        "            border-radius: 5px;\n",
        "            border: 1px solid #dee2e6;\n",
        "            font-family: 'Courier New', monospace;\n",
        "            font-size: 9pt;\n",
        "        }\n",
        "\n",
        "        blockquote {\n",
        "            background: #f0f8ff;\n",
        "            border-left: 4px solid #3498db;\n",
        "            margin: 15px 0;\n",
        "            padding: 10px 15px;\n",
        "            color: #555;\n",
        "            font-style: italic;\n",
        "        }\n",
        "\n",
        "        li { margin-bottom: 5px; }\n",
        "        .page-break { page-break-after: always; }\n",
        "    \"\"\")\n",
        "\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    output_filename = os.path.join(output_folder, f\"Study_Guide_{timestamp}.pdf\")\n",
        "\n",
        "    HTML(string=html_content, base_url='.').write_pdf(output_filename, stylesheets=[css])\n",
        "    print(f\"üéâ Guide Saved: {output_filename}\")\n",
        "\n",
        "run_pipeline()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dmXLOKagcKfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}