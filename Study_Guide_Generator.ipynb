{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMae1pUMDBSSnMteFb9uSNB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "37fc6bbc5fc545e48ce4b04627ea9e6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_404aca1347324a4bad6d0d073e32c579",
              "IPY_MODEL_5a8d8ba35b00408dae8ca45ce023d286",
              "IPY_MODEL_fffcc476e97b4935ac408b1ebcf08d4d"
            ],
            "layout": "IPY_MODEL_13c2b76fd42a4653aae07e8cbc4082bc"
          }
        },
        "404aca1347324a4bad6d0d073e32c579": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a84b243a90bd46029d432220beda6d7d",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_72d8417217be4e79adc708a9a8723dfa",
            "value": "Loading‚Äácheckpoint‚Äáshards:‚Äá100%"
          }
        },
        "5a8d8ba35b00408dae8ca45ce023d286": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2f0b3a1ec8784f2d994d67135ff53d3f",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f390fbbf7a2d4ccfb6690cded1a022f1",
            "value": 4
          }
        },
        "fffcc476e97b4935ac408b1ebcf08d4d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc16a6532460492a8d0384d1cbe029ee",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_276a4d9e4190411aa41761e8f9c15bd3",
            "value": "‚Äá4/4‚Äá[01:15&lt;00:00,‚Äá16.43s/it]"
          }
        },
        "13c2b76fd42a4653aae07e8cbc4082bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a84b243a90bd46029d432220beda6d7d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "72d8417217be4e79adc708a9a8723dfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2f0b3a1ec8784f2d994d67135ff53d3f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f390fbbf7a2d4ccfb6690cded1a022f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc16a6532460492a8d0384d1cbe029ee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "276a4d9e4190411aa41761e8f9c15bd3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pakeetharan/ai-study-guide/blob/main/Study_Guide_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö AI Study Guide Generator\n",
        "**Turn your lecture slides and textbooks into professional exam notes and practice questions.**\n",
        "\n",
        "### **How to use this tool:**\n",
        "1. **Check Settings:** Go to top menu `Runtime` -> `Change runtime type` and ensure **T4 GPU** is selected.\n",
        "2. **Initialize:** Click the **Play** button on **Step 1** below. Wait for it to say \"System Ready\" (~2 mins).\n",
        "3. **Upload & Run:** Click the **Play** button on **Step 2**.\n",
        "    * You will be asked to connect to **Google Drive** (this is to safely save your final PDF).\n",
        "    * Click **\"Choose Files\"** to upload your PDFs. You can upload multiple files (e.g., *Week1.pdf, Week2.pdf*) at once.\n",
        "4. **Get Results:** The AI will analyze each document separately and save a `Study_Guide_TIMESTAMP.pdf` into your Google Drive folder: `My Drive > AI_Study_Notes`.\n",
        "\n",
        "---\n",
        "**üí° Pro Tip:** Upload separate PDF files for each lecture topic instead of merging them. This helps the AI generate specific practice questions for every single topic."
      ],
      "metadata": {
        "id": "1Gm2N9Znbrc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üöÄ Step 1: Initialize System\n",
        "# @markdown Installs the AI engine, OCR tools, and PDF processors. It takes about **2 minutes**.\n",
        "# @markdown You only need to run this once per session.\n",
        "\n",
        "import os, sys, subprocess\n",
        "import logging, warnings\n",
        "\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"‚è≥ Installing System Dependencies & Fonts...\")\n",
        "with open(os.devnull, 'w') as devnull:\n",
        "    subprocess.run([\"apt-get\", \"update\"], stdout=devnull, stderr=devnull)\n",
        "    # Added 'fonts-roboto' for better typography\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"tesseract-ocr\", \"poppler-utils\",\n",
        "                    \"libcairo2\", \"libpango-1.0-0\", \"libgdk-pixbuf2.0-0\", \"libffi-dev\",\n",
        "                    \"fonts-roboto\"], stdout=devnull, stderr=devnull)\n",
        "\n",
        "    pkgs = [\n",
        "        \"transformers\", \"accelerate\", \"bitsandbytes\", \"langchain-huggingface\",\n",
        "        \"langchain-text-splitters\", \"langchain-community\", \"langchain-core\",\n",
        "        \"pdfplumber\", \"pdf2image\", \"pytesseract\", \"markdown\", \"weasyprint\",\n",
        "        \"tiktoken\", \"tqdm\", \"numpy\"\n",
        "    ]\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs, stdout=devnull, stderr=devnull)\n",
        "\n",
        "import torch\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "import markdown\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from pdf2image import convert_from_path\n",
        "from google.colab import files, drive\n",
        "from weasyprint import HTML, CSS\n",
        "from weasyprint.text.fonts import FontConfiguration\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "print(\"‚è≥ Loading Llama-3-8B (Context-Aware Mode)...\")\n",
        "model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048, model_kwargs={\"temperature\": 0.3}, return_full_text=False)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"‚úÖ System Ready.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E8Z2sbtTb67n",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "37fc6bbc5fc545e48ce4b04627ea9e6e",
            "404aca1347324a4bad6d0d073e32c579",
            "5a8d8ba35b00408dae8ca45ce023d286",
            "fffcc476e97b4935ac408b1ebcf08d4d",
            "13c2b76fd42a4653aae07e8cbc4082bc",
            "a84b243a90bd46029d432220beda6d7d",
            "72d8417217be4e79adc708a9a8723dfa",
            "2f0b3a1ec8784f2d994d67135ff53d3f",
            "f390fbbf7a2d4ccfb6690cded1a022f1",
            "dc16a6532460492a8d0384d1cbe029ee",
            "276a4d9e4190411aa41761e8f9c15bd3"
          ]
        },
        "outputId": "d0fc9720-c7d8-4467-a134-4d5e4f0a375d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Installing System Dependencies & Fonts...\n",
            "‚è≥ Loading Llama-3-8B (Context-Aware Mode)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "37fc6bbc5fc545e48ce4b04627ea9e6e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ System Ready.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gc"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4iqw5PcbLKAy",
        "outputId": "f54d557b-02ab-409d-b4a3-0c0c694ee08d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement gc (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for gc\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìÇ Step 2: Upload Files & Generate Guide\n",
        "# @markdown **Instructions:**\n",
        "# @markdown 1. Run this cell to connect to Drive.\n",
        "# @markdown 2. Upload your PDFs when the button appears.\n",
        "# @markdown 3. The AI will process each file and save the result to `My Drive > AI_Study_Notes`.\n",
        "\n",
        "import os\n",
        "from google.colab import drive, files\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# --- 1. Drive Connection ---\n",
        "print(\"üîå Checking Google Drive connection...\")\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"‚úÖ Drive is already connected.\")\n",
        "\n",
        "output_folder = \"/content/drive/My Drive/AI_Study_Notes\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "generate_exercises = True # @param {type:\"boolean\"}\n",
        "OPTIMAL_CHUNK_SIZE = 7000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "def extract_text_from_file(filename):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(filename) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                extracted = page.extract_text()\n",
        "                if extracted: text += extracted + \"\\n\"\n",
        "        if len(text) < 500: # OCR Fallback\n",
        "            print(f\"   ‚ö†Ô∏è Scanned content detected in {filename}. Running OCR...\")\n",
        "            images = convert_from_path(filename)\n",
        "            for img in images: text += pytesseract.image_to_string(img) + \"\\n\"\n",
        "    except Exception as e: print(f\"   ‚ùå Error reading {filename}: {e}\")\n",
        "    return text\n",
        "\n",
        "def run_pipeline():\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"   ‚¨áÔ∏è  CLICK THE BUTTON BELOW TO UPLOAD  ‚¨áÔ∏è\")\n",
        "    print(\"=\"*40)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No files uploaded.\")\n",
        "        return\n",
        "\n",
        "    all_notes_markdown = \"\"\n",
        "    all_exercises_markdown = \"\"\n",
        "\n",
        "    for i, filename in enumerate(uploaded.keys()):\n",
        "        print(f\"\\nüöÄ Processing File {i+1}/{len(uploaded)}: {filename}...\")\n",
        "\n",
        "        # Memory Cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        raw_text = extract_text_from_file(filename)\n",
        "        if not raw_text.strip(): continue\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "            tokenizer, chunk_size=OPTIMAL_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "        docs = splitter.create_documents([raw_text])\n",
        "\n",
        "        # --- Context-Aware Notes ---\n",
        "        print(f\"   üìù Generating Notes ({len(docs)} sections)...\")\n",
        "        # Removed emoji from header to prevent box character in PDF\n",
        "        file_notes = f\"# Module: {filename}\\n\"\n",
        "\n",
        "        note_prompt = PromptTemplate.from_template(\n",
        "            \"\"\"\n",
        "            You are an expert Professor. Analyze this text section:\n",
        "            \"{text}\"\n",
        "\n",
        "            TRANSFORM THIS INTO STUDY NOTES.\n",
        "\n",
        "            Formatting Rules:\n",
        "            1. **Comparisons:** If comparing items, create a Markdown Table.\n",
        "            2. **Formulas:** Use Code Blocks (```) for math.\n",
        "            3. **Concepts:** Use bold headers.\n",
        "            4. **Summary:** End with a bullet-point summary.\n",
        "\n",
        "            Output strictly in Markdown.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        for doc_idx, doc in enumerate(tqdm(docs, desc=\"   > Analyzing\", leave=False)):\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": note_prompt.format(text=doc.page_content)}]\n",
        "                fmt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "                outputs = pipe(fmt, max_new_tokens=1500, pad_token_id=tokenizer.eos_token_id)\n",
        "                res = outputs[0][\"generated_text\"]\n",
        "\n",
        "                clean_res = res.split(\"assistant\")[-1].strip() if \"assistant\" in res else res\n",
        "                file_notes += f\"\\n{clean_res}\\n\"\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n   ‚ùå Error on Section {doc_idx}: {str(e)}\")\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        all_notes_markdown += file_notes + \"\\n\\n<div class='page-break'></div>\\n\\n\"\n",
        "\n",
        "        if generate_exercises:\n",
        "            print(f\"   üß† Designing Practice Questions...\")\n",
        "            mid = len(raw_text) // 4\n",
        "            sample_context = raw_text[mid : mid + OPTIMAL_CHUNK_SIZE]\n",
        "\n",
        "            # UPDATED PROMPT: Explicitly asks for vertical list formatting\n",
        "            ex_prompt = f\"\"\"\n",
        "            Create an Exam Section based on:\n",
        "            \"{sample_context}\"\n",
        "\n",
        "            Requirements:\n",
        "            1. **3 Multiple Choice Questions.** - CRITICAL: Format options on new lines.\n",
        "               - Example:\n",
        "                 1. Question?\n",
        "                    a) Option\n",
        "                    b) Option\n",
        "\n",
        "            2. **2 Short Answer Questions.**\n",
        "\n",
        "            3. **Answer Key:**\n",
        "               - Format as Blockquote (>).\n",
        "               - Example: > 1. a) Explanation...\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": ex_prompt}]\n",
        "                fmt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "                res = pipe(fmt, max_new_tokens=1500, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"].split(\"assistant\")[-1].strip()\n",
        "                # Removed emoji from header\n",
        "                all_exercises_markdown += f\"## Practice: {filename}\\n{res}\\n\\n<div class='page-break'></div>\\n\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error generating exercises: {e}\")\n",
        "\n",
        "    # --- PDF Rendering ---\n",
        "    print(\"\\nüíæ Rendering Professional PDF...\")\n",
        "    final_md = f\"\"\"\n",
        "    {all_notes_markdown}\n",
        "    # Part 2: Practice Workbook\n",
        "    {all_exercises_markdown}\n",
        "    \"\"\"\n",
        "\n",
        "    html_content = markdown.markdown(final_md, extensions=['extra', 'codehilite', 'tables', 'fenced_code'])\n",
        "\n",
        "    # CSS Updates:\n",
        "    # 1. Removed @import (uses local fonts).\n",
        "    # 2. Added specific styling for lists (li) to fix MCQ bunching.\n",
        "    css = CSS(string=\"\"\"\n",
        "        @page { size: A4; margin: 2cm; }\n",
        "        body {\n",
        "            font-family: 'Roboto', 'Helvetica', 'Arial', sans-serif;\n",
        "            font-size: 11pt;\n",
        "            line-height: 1.6;\n",
        "            color: #2d3436;\n",
        "        }\n",
        "        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; margin-top: 40px; font-weight: 700;}\n",
        "        h2 { color: #e67e22; margin-top: 25px; font-weight: 400; border-left: 5px solid #e67e22; padding-left: 10px;}\n",
        "        strong { color: #2980b9; }\n",
        "\n",
        "        /* List Styling for MCQs */\n",
        "        ul, ol { margin-bottom: 15px; padding-left: 20px; }\n",
        "        li { margin-bottom: 5px; }\n",
        "\n",
        "        table { width: 100%; border-collapse: collapse; margin: 20px 0; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n",
        "        th { background-color: #34495e; color: white; padding: 12px; text-align: left; }\n",
        "        td { border: 1px solid #dfe6e9; padding: 10px; }\n",
        "        tr:nth-child(even) { background-color: #f1f2f6; }\n",
        "\n",
        "        pre { background-color: #f5f6fa; border: 1px solid #dcdde1; border-radius: 5px; padding: 15px; font-family: 'Courier New', monospace; }\n",
        "        blockquote { background: #f0f8ff; border-left: 5px solid #3498db; margin: 10px 0; padding: 10px 20px; color: #555; }\n",
        "        .page-break { page-break-after: always; }\n",
        "    \"\"\")\n",
        "\n",
        "    font_config = FontConfiguration()\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    output_filename = os.path.join(output_folder, f\"Study_Guide_{timestamp}.pdf\")\n",
        "\n",
        "    HTML(string=html_content, base_url='.').write_pdf(output_filename, stylesheets=[css], font_config=font_config)\n",
        "    print(f\"üéâ Guide Saved: {output_filename}\")\n",
        "\n",
        "run_pipeline()"
      ],
      "metadata": {
        "id": "dmXLOKagcKfF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "8b738350-047a-417e-a0e6-edd055daf7e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîå Checking Google Drive connection...\n",
            "‚úÖ Drive is already connected.\n",
            "\n",
            "========================================\n",
            "   ‚¨áÔ∏è  CLICK THE BUTTON BELOW TO UPLOAD  ‚¨áÔ∏è\n",
            "========================================\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b1e49a47-0a40-4dd8-aff4-72afc9db3dfa\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b1e49a47-0a40-4dd8-aff4-72afc9db3dfa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving CogSys_MCS4201_Note01.pdf to CogSys_MCS4201_Note01 (4).pdf\n",
            "Saving CogSys_MCS4201_Note02.pdf to CogSys_MCS4201_Note02 (4).pdf\n",
            "\n",
            "üöÄ Processing File 1/2: CogSys_MCS4201_Note01 (4).pdf...\n",
            "   üìù Generating Notes (1 sections)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                     "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "   ‚ùå Error on Section 0: CUDA out of memory. Tried to allocate 2.90 GiB. GPU 0 has a total capacity of 14.74 GiB of which 1.56 GiB is free. Process 2478 has 13.18 GiB memory in use. Of the allocated memory 8.79 GiB is allocated by PyTorch, and 4.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
            "   üß† Designing Practice Questions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}