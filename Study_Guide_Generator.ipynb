{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyO0dd5Vw3HxB7GUq1tuBzXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pakeetharan/ai-study-guide/blob/main/Study_Guide_Generator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# üìö AI Study Guide Generator\n",
        "**Turn your lecture slides and textbooks into professional exam notes and practice questions.**\n",
        "\n",
        "### **How to use this tool:**\n",
        "1. **Check Settings:** Go to top menu `Runtime` -> `Change runtime type` and ensure **T4 GPU** is selected.\n",
        "2. **Initialize:** Click the **Play** button on **Step 1** below. Wait for it to say \"System Ready\" (~2 mins).\n",
        "3. **Upload & Run:** Click the **Play** button on **Step 2**.\n",
        "    * You will be asked to connect to **Google Drive** (this is to safely save your final PDF).\n",
        "    * Click **\"Choose Files\"** to upload your PDFs. You can upload multiple files (e.g., *Week1.pdf, Week2.pdf*) at once.\n",
        "4. **Get Results:** The AI will analyze each document separately and save a `Study_Guide_TIMESTAMP.pdf` into your Google Drive folder: `My Drive > AI_Study_Notes`.\n",
        "\n",
        "---\n",
        "**üí° Pro Tip:** Upload separate PDF files for each lecture topic instead of merging them. This helps the AI generate specific practice questions for every single topic."
      ],
      "metadata": {
        "id": "1Gm2N9Znbrc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üöÄ Step 1: Initialize System\n",
        "# @markdown Installs the AI engine, OCR tools, and PDF processors. It takes about **2 minutes**.\n",
        "# @markdown You only need to run this once per session.\n",
        "\n",
        "import os, sys, subprocess\n",
        "import logging, warnings\n",
        "\n",
        "logging.getLogger(\"pdfminer\").setLevel(logging.ERROR)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "print(\"‚è≥ Installing System Dependencies & Fonts...\")\n",
        "with open(os.devnull, 'w') as devnull:\n",
        "    subprocess.run([\"apt-get\", \"update\"], stdout=devnull, stderr=devnull)\n",
        "    # Added 'fonts-roboto' for better typography\n",
        "    subprocess.run([\"apt-get\", \"install\", \"-y\", \"tesseract-ocr\", \"poppler-utils\",\n",
        "                    \"libcairo2\", \"libpango-1.0-0\", \"libgdk-pixbuf2.0-0\", \"libffi-dev\",\n",
        "                    \"fonts-roboto\"], stdout=devnull, stderr=devnull)\n",
        "\n",
        "    pkgs = [\n",
        "        \"transformers\", \"accelerate\", \"bitsandbytes\", \"langchain-huggingface\",\n",
        "        \"langchain-text-splitters\", \"langchain-community\", \"langchain-core\",\n",
        "        \"pdfplumber\", \"pdf2image\", \"pytesseract\", \"markdown\", \"weasyprint\",\n",
        "        \"tiktoken\", \"tqdm\", \"numpy\"\n",
        "    ]\n",
        "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\"] + pkgs, stdout=devnull, stderr=devnull)\n",
        "\n",
        "import torch\n",
        "import pdfplumber\n",
        "import pytesseract\n",
        "import markdown\n",
        "from datetime import datetime\n",
        "from tqdm import tqdm\n",
        "from pdf2image import convert_from_path\n",
        "from google.colab import files, drive\n",
        "from weasyprint import HTML, CSS\n",
        "from weasyprint.text.fonts import FontConfiguration\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
        "from langchain_huggingface import HuggingFacePipeline\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "print(\"‚è≥ Loading Llama-3-8B (Context-Aware Mode)...\")\n",
        "model_id = \"NousResearch/Meta-Llama-3-8B-Instruct\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True, bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map=\"auto\")\n",
        "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=2048, model_kwargs={\"temperature\": 0.3}, return_full_text=False)\n",
        "llm = HuggingFacePipeline(pipeline=pipe)\n",
        "\n",
        "print(\"‚úÖ System Ready.\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "E8Z2sbtTb67n",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title üìÇ Step 2: Upload Files & Generate Guide\n",
        "# @markdown **Instructions:**\n",
        "# @markdown 1. Run this cell to connect to Drive.\n",
        "# @markdown 2. Upload your PDFs when the button appears.\n",
        "# @markdown 3. The AI will process each file and save the result to `My Drive > AI_Study_Notes`.\n",
        "\n",
        "import os\n",
        "from google.colab import drive, files\n",
        "import gc\n",
        "import torch\n",
        "\n",
        "# --- 1. Drive Connection ---\n",
        "print(\"üîå Checking Google Drive connection...\")\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"‚úÖ Drive is already connected.\")\n",
        "\n",
        "output_folder = \"/content/drive/My Drive/AI_Study_Notes\"\n",
        "if not os.path.exists(output_folder):\n",
        "    os.makedirs(output_folder)\n",
        "\n",
        "# --- 2. Configuration ---\n",
        "generate_exercises = True # @param {type:\"boolean\"}\n",
        "OPTIMAL_CHUNK_SIZE = 7000\n",
        "CHUNK_OVERLAP = 200\n",
        "\n",
        "def extract_text_from_file(filename):\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(filename) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                extracted = page.extract_text()\n",
        "                if extracted: text += extracted + \"\\n\"\n",
        "        if len(text) < 500: # OCR Fallback\n",
        "            print(f\"   ‚ö†Ô∏è Scanned content detected in {filename}. Running OCR...\")\n",
        "            images = convert_from_path(filename)\n",
        "            for img in images: text += pytesseract.image_to_string(img) + \"\\n\"\n",
        "    except Exception as e: print(f\"   ‚ùå Error reading {filename}: {e}\")\n",
        "    return text\n",
        "\n",
        "def run_pipeline():\n",
        "    print(\"\\n\" + \"=\"*40)\n",
        "    print(\"   ‚¨áÔ∏è  CLICK THE BUTTON BELOW TO UPLOAD  ‚¨áÔ∏è\")\n",
        "    print(\"=\"*40)\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if not uploaded:\n",
        "        print(\"‚ùå No files uploaded.\")\n",
        "        return\n",
        "\n",
        "    all_notes_markdown = \"\"\n",
        "    all_exercises_markdown = \"\"\n",
        "\n",
        "    for i, filename in enumerate(uploaded.keys()):\n",
        "        print(f\"\\nüöÄ Processing File {i+1}/{len(uploaded)}: {filename}...\")\n",
        "\n",
        "        # Memory Cleanup\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "\n",
        "        raw_text = extract_text_from_file(filename)\n",
        "        if not raw_text.strip(): continue\n",
        "\n",
        "        splitter = RecursiveCharacterTextSplitter.from_huggingface_tokenizer(\n",
        "            tokenizer, chunk_size=OPTIMAL_CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP\n",
        "        )\n",
        "        docs = splitter.create_documents([raw_text])\n",
        "\n",
        "        # --- Context-Aware Notes ---\n",
        "        print(f\"   üìù Generating Notes ({len(docs)} sections)...\")\n",
        "        # Removed emoji from header to prevent box character in PDF\n",
        "        file_notes = f\"# Module: {filename}\\n\"\n",
        "\n",
        "        note_prompt = PromptTemplate.from_template(\n",
        "            \"\"\"\n",
        "            You are an expert Professor. Analyze this text section:\n",
        "            \"{text}\"\n",
        "\n",
        "            TRANSFORM THIS INTO STUDY NOTES.\n",
        "\n",
        "            Formatting Rules:\n",
        "            1. **Comparisons:** If comparing items, create a Markdown Table.\n",
        "            2. **Formulas:** Use Code Blocks (```) for math.\n",
        "            3. **Concepts:** Use bold headers.\n",
        "            4. **Summary:** End with a bullet-point summary.\n",
        "\n",
        "            Output strictly in Markdown.\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        for doc_idx, doc in enumerate(tqdm(docs, desc=\"   > Analyzing\", leave=False)):\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": note_prompt.format(text=doc.page_content)}]\n",
        "                fmt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "                outputs = pipe(fmt, max_new_tokens=1500, pad_token_id=tokenizer.eos_token_id)\n",
        "                res = outputs[0][\"generated_text\"]\n",
        "\n",
        "                clean_res = res.split(\"assistant\")[-1].strip() if \"assistant\" in res else res\n",
        "                file_notes += f\"\\n{clean_res}\\n\"\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\n   ‚ùå Error on Section {doc_idx}: {str(e)}\")\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        all_notes_markdown += file_notes + \"\\n\\n<div class='page-break'></div>\\n\\n\"\n",
        "\n",
        "        if generate_exercises:\n",
        "            print(f\"   üß† Designing Practice Questions...\")\n",
        "            mid = len(raw_text) // 4\n",
        "            sample_context = raw_text[mid : mid + OPTIMAL_CHUNK_SIZE]\n",
        "\n",
        "            # UPDATED PROMPT: Explicitly asks for vertical list formatting\n",
        "            ex_prompt = f\"\"\"\n",
        "            Create an Exam Section based on:\n",
        "            \"{sample_context}\"\n",
        "\n",
        "            Requirements:\n",
        "            1. **3 Multiple Choice Questions.** - CRITICAL: Format options on new lines.\n",
        "               - Example:\n",
        "                 1. Question?\n",
        "                    a) Option\n",
        "                    b) Option\n",
        "\n",
        "            2. **2 Short Answer Questions.**\n",
        "\n",
        "            3. **Answer Key:**\n",
        "               - Format as Blockquote (>).\n",
        "               - Example: > 1. a) Explanation...\n",
        "            \"\"\"\n",
        "\n",
        "            try:\n",
        "                messages = [{\"role\": \"user\", \"content\": ex_prompt}]\n",
        "                fmt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "                res = pipe(fmt, max_new_tokens=1500, pad_token_id=tokenizer.eos_token_id)[0][\"generated_text\"].split(\"assistant\")[-1].strip()\n",
        "                # Removed emoji from header\n",
        "                all_exercises_markdown += f\"## Practice: {filename}\\n{res}\\n\\n<div class='page-break'></div>\\n\\n\"\n",
        "            except Exception as e:\n",
        "                print(f\"   ‚ùå Error generating exercises: {e}\")\n",
        "\n",
        "    # --- PDF Rendering ---\n",
        "    print(\"\\nüíæ Rendering Professional PDF...\")\n",
        "    final_md = f\"\"\"\n",
        "    {all_notes_markdown}\n",
        "    # Part 2: Practice Workbook\n",
        "    {all_exercises_markdown}\n",
        "    \"\"\"\n",
        "\n",
        "    html_content = markdown.markdown(final_md, extensions=['extra', 'codehilite', 'tables', 'fenced_code'])\n",
        "\n",
        "    # CSS Updates:\n",
        "    # 1. Removed @import (uses local fonts).\n",
        "    # 2. Added specific styling for lists (li) to fix MCQ bunching.\n",
        "    css = CSS(string=\"\"\"\n",
        "        @page { size: A4; margin: 2cm; }\n",
        "        body {\n",
        "            font-family: 'Roboto', 'Helvetica', 'Arial', sans-serif;\n",
        "            font-size: 11pt;\n",
        "            line-height: 1.6;\n",
        "            color: #2d3436;\n",
        "        }\n",
        "        h1 { color: #2c3e50; border-bottom: 3px solid #3498db; padding-bottom: 10px; margin-top: 40px; font-weight: 700;}\n",
        "        h2 { color: #e67e22; margin-top: 25px; font-weight: 400; border-left: 5px solid #e67e22; padding-left: 10px;}\n",
        "        strong { color: #2980b9; }\n",
        "\n",
        "        /* List Styling for MCQs */\n",
        "        ul, ol { margin-bottom: 15px; padding-left: 20px; }\n",
        "        li { margin-bottom: 5px; }\n",
        "\n",
        "        table { width: 100%; border-collapse: collapse; margin: 20px 0; box-shadow: 0 2px 5px rgba(0,0,0,0.1); }\n",
        "        th { background-color: #34495e; color: white; padding: 12px; text-align: left; }\n",
        "        td { border: 1px solid #dfe6e9; padding: 10px; }\n",
        "        tr:nth-child(even) { background-color: #f1f2f6; }\n",
        "\n",
        "        pre { background-color: #f5f6fa; border: 1px solid #dcdde1; border-radius: 5px; padding: 15px; font-family: 'Courier New', monospace; }\n",
        "        blockquote { background: #f0f8ff; border-left: 5px solid #3498db; margin: 10px 0; padding: 10px 20px; color: #555; }\n",
        "        .page-break { page-break-after: always; }\n",
        "    \"\"\")\n",
        "\n",
        "    font_config = FontConfiguration()\n",
        "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "    output_filename = os.path.join(output_folder, f\"Study_Guide_{timestamp}.pdf\")\n",
        "\n",
        "    HTML(string=html_content, base_url='.').write_pdf(output_filename, stylesheets=[css], font_config=font_config)\n",
        "    print(f\"üéâ Guide Saved: {output_filename}\")\n",
        "\n",
        "run_pipeline()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "dmXLOKagcKfF"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}